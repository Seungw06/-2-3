{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "model.predict(np.array([[0.5, 0.0, 0.1, 0.0, 0.0, 0.4, 0.0, 0.0, 0.1, 0.0, 0.0]]))\n",
    "model.predict(np.array([0.5, 0.0, 0.1, 0.0, 0.0, 0.4, 0.0, 0.0, 0.1, 0.0, 0.0]))\n",
    "\n",
    "#from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input , BatchNormalization, Conv2D, Activation\n",
    "from keras.laters import MaxPool2D, Flatten, Dense, Add\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters):\n",
    "    \n",
    "    x = BatchNormalization() (x)\n",
    "    x = Conv2D(filters, (3,3), activation='relu', padding = 'same') (x)\n",
    "    \n",
    "    x = BatchNormalization() (x)\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, (3, 3), activation='relu', padding = 'same') (x)\n",
    "    \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model(input_shape, n_classes):\n",
    "    \n",
    "    input_tesor = Input(shape=input_shape)\n",
    "    \n",
    "    x = conv_block(input_tensor, 32)\n",
    "    x = conv_block(x, 64)\n",
    "    x = conv_block(x, 128)\n",
    "    x = conv_block(x, 256)\n",
    "    x = conv_block(x, 512)\n",
    "    \n",
    "    x = Flatten() (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Dense(512, activation='relu') (x)\n",
    "    x = Dense(512, activation='relu') (x)\n",
    "    \n",
    "    output_layer = Dense(n_classes, activation='softmax') (x)\n",
    "    \n",
    "    input = [input_tensor]\n",
    "    model = Model(inputs, output_layer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_height = 48\n",
    "input_width = 48\n",
    "input_channel = 3\n",
    "\n",
    "input_shape = (input_height, input_width, input_channel)\n",
    "n_classes = 6\n",
    "\n",
    "data_dir = './data'\n",
    "data1_path = os.path.join(data_dir,'left', '*.jpg')\n",
    "data1_files = glob.glob(data1_path)\n",
    "data2_path = os.path.join(data_dir,'right', '*.jpg')\n",
    "data2_files = glob.glob(data2_path)\n",
    "data3_path = os.path.join(data_dir,'stop', '*.jpg')\n",
    "data3_files = glob.glob(data3_path)\n",
    "data4_path = os.path.join(data_dir,'others', '*.jpg')\n",
    "data4_files = glob.glob(data4_path)\n",
    "data5_path = os.path.join(data_dir,'forward', '*.jpg')\n",
    "data5_files = glob.glob(data5_path)\n",
    "data6_path = os.path.join(data_dir,'uturn', '*.jpg')\n",
    "data6_files = glob.glob(data6_path)\n",
    "\n",
    "test_path = os.path.join(data_dir,'test', '*.jpg')\n",
    "test_files = glob.glob(test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_train = len(data1_files) + len(data2_files) \\\n",
    "        + len(data3_files) + len(data4_files) \\\n",
    "        + len(data5_files) + len(data6_files)\n",
    "n_test = len(test_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = np.zeros(\n",
    "    shape=(n_train, input_width,\n",
    "          input_channel), dtype='float32')\n",
    "\n",
    "label = np.zeros(\n",
    "    shape=(n_train, n_classes), dtype='float32')\n",
    "\n",
    "testset = np.zeros(\n",
    "    shape=(n_test, input_height, input_width,input_channel), dtype='float32')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = data1_files + data2_files + data3_files \\\n",
    "            + data4_files + data5_files + data6_files\n",
    "\n",
    "for ind , file in enumerate(train_files):\n",
    "    try:\n",
    "        image = cv2.imread(file)\n",
    "        resized_image = cv2.resized(image,\n",
    "                                   (input_width, input_height))\n",
    "        trainset[ind] = resized_image\n",
    "    except Exception as e:\n",
    "        print(file)\n",
    "\n",
    "\n",
    "for ind, file in enumerate(test_file):\n",
    "    try:\n",
    "        image = cv2.imread(file)\n",
    "        resized_image = cv2.resize(image\n",
    "                                  (input_width, input_height))\n",
    "        testset[ind] = resized_image\n",
    "    except Exception as e:\n",
    "        print(file)\n",
    "\n",
    "trainset = trainset / 255.0\n",
    "testset = testest / 255.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data1 = len(data1_files)\n",
    "n_data2 = len(data2_files)\n",
    "n_data3 = len(data3_files)\n",
    "n_data4 = len(data4_files)\n",
    "n_data5 = len(data5_files)\n",
    "n_data6 = len(data6_files)\n",
    "\n",
    "\n",
    "begin_ind = 0\n",
    "end_ind = n_data1\n",
    "label[begin_ind:end_ind, 0] = 1.0\n",
    "\n",
    "begin_ind = n_data1\n",
    "end_ind = n_data1 + n_data2\n",
    "label[begin_ind:end_ind, 1] = 1.0\n",
    "\n",
    "begin_ind = n_data1 + n_data2\n",
    "end_ind = n_data1 + n_data2 + n_data3\n",
    "label[begin_ind:end_ind, 2] = 1.0\n",
    "\n",
    "begin_ind = n_data1 + n_data2 + n_data3\n",
    "end_ind = n_data1 + n_data2 + n_data3 + n_data4\n",
    "label[begin_ind:end_ind, 3] = 1.0\n",
    "\n",
    "begin_ind = n_data1 + n_data2 + n_data3 + n_data4\n",
    "end_ind = n_data1 + n_data2 + n_data3 + n_data4 + n_data5\n",
    "label[begin_ind:end_ind, 4] = 1.0\n",
    "\n",
    "begin_ind = n_data1 + n_data2 + n_data3 + n_data4 + n_data5\n",
    "end_ind = n_data1 + n_data2 + n_data3 + n_data4 + n_data5 + n_data6\n",
    "label[begin_ind:end_ind, 5] = 1.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #모델함수 호출\n",
    "    model = custom_model(input_shape, n_classes)\n",
    "    #인공지능 모델을 이용한 최적 데이타 학습방법 설계\n",
    "    adam = Adam()\n",
    "    model.compile(\n",
    "        optimizer = adam,\n",
    "        loss = 'categorical_croseentropy',\n",
    "        metrics = ['accuracy'])\n",
    "    #인공지능 모델을 이용한 데이타 학습실행\n",
    "    history = model.fit(\n",
    "        trainset, label,\n",
    "        batch_size = 20,\n",
    "        epochs = 8,\n",
    "        validation_split = 0.05)\n",
    "    #모델파일 저장\n",
    "    #가중치 파일 저장\n",
    "    model_desc = model.to_json()\n",
    "    with open('./model/model.json', 'w') as file_model:\n",
    "        file_model.write(model_desc)\n",
    "    model.save_weights('./model/weights.h5')\n",
    "    \n",
    "    #평가데이타를 이용한 예측 정확도 확인\n",
    "    if testest.shape[0] != 0:\n",
    "        result_oneshot = model.predict(testset)\n",
    "        result_predict = np.argmax(result_oneshot,axis=1)\n",
    "    else:\n",
    "        result_sparse = list()\n",
    "        \n",
    "    print('file name \\t forecast category')\n",
    "    \n",
    "    #평가 데이타 파일면에 따른 예측 결과값 출력\n",
    "    for file, label_id in zip(test_files, result_predict):\n",
    "        filename = os.path.basename(file)\n",
    "        \n",
    "        if label_id == 0:\n",
    "            label_name = 'left'\n",
    "        elif label_id == 1:\n",
    "            label_name = 'right'\n",
    "        elif label_id == 2:\n",
    "            label_name = 'stop'\n",
    "        elif label_id == 3:\n",
    "            label_name = 'others'\n",
    "        elif label_id == 4:\n",
    "            label_name = 'forward'\n",
    "        elif label_id == 5:\n",
    "            label_name = 'uturn'\n",
    "        \n",
    "        print('%s\\t%s' % (filename, label_name))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        #모델 학습진행 결과 출력\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        \n",
    "        epochs = range(len(acc))\n",
    "        \n",
    "        plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "        plt.plot(epochs, val_acc, 'b', label = 'validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.figure()\n",
    "        \n",
    "        plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label = 'validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
